
[User Input]: Our Kubernetes pods are stuck in CrashLoopBackOff after a new deployment.
[Response]: Okay, as an SRE Expert, I will analyze the "Kubernetes pods stuck in `CrashLoopBackOff` after a new deployment" incident and provide a professional, actionable troubleshooting plan.

---

### Incident Analysis: Kubernetes Pods in `CrashLoopBackOff` After New Deployment

The `CrashLoopBackOff` status indicates that a container within a pod is starting, crashing, and then Kubernetes is attempting to restart it repeatedly. The crucial context here is "after a new deployment," which strongly suggests that the issue is related to changes introduced by this deployment.

**Possible Causes:**

1.  **Application-level Issues:**
    *   **Configuration Errors:** Incorrect environment variables, missing configuration files, malformed configurations, incorrect database connection strings, or API endpoints. This is a very common cause for crashes immediately after deployment.
    *   **Code Bugs:** A newly introduced bug in the application code that causes it to fail during startup or initialization.
    *   **Missing Dependencies:** The application expects certain files, libraries, or external services that are not available or incorrectly configured in the new environment/image.
    *   **Incorrect Startup Command/Arguments:** The `command` or `args` defined in the pod specification are incorrect or lead to an unhandled error.

2.  **Resource-related Issues:**
    *   **OOMKilled (Out Of Memory):** The container tries to allocate more memory than specified in its `resources.limits.memory` and is killed by the Kubernetes node's OOM killer. This is a very frequent cause of `CrashLoopBackOff`.
    *   **CPU Throttling:** While less likely to cause an immediate crash, severe CPU throttling (due to low `resources.limits.cpu`) could prevent the application from initializing within a given timeout, leading to probe failures or unresponsiveness.

3.  **Kubernetes Configuration Issues (Deployment Spec):**
    *   **Incorrect Image:** The deployed image tag is wrong, corrupted, or incompatible with the application code or its dependencies.
    *   **Volume Mount Errors:** Issues with mounting `ConfigMaps`, `Secrets`, Persistent Volumes (PVs), or HostPath volumes, preventing the application from accessing critical data or configuration.
    *   **Liveness Probe Misconfiguration:** The liveness probe is too aggressive, failing prematurely before the application has had enough time to start up, causing Kubernetes to restart the healthy-but-initializing container.
    *   **Service Mesh Sidecar Injection:** If using a service mesh (e.g., Istio, Linkerd), issues with the injected sidecar proxy (e.g., proxy failing to start, conflicting with app) can cause the main application container to crash.

4.  **External Dependency Issues:**
    *   **Database Unavailable/Unreachable:** The application fails to connect to its database during startup.
    *   **External API Service Issues:** Critical external services that the application relies on for initialization are down or unreachable.
    *   **Network Policy:** New or modified Kubernetes `NetworkPolicies` might be preventing the pod from communicating with necessary internal or external services.

---

### Actionable Troubleshooting Steps

We'll follow a systematic approach, starting with the most common and easiest checks.

**Phase 1: Initial Triage & Information Gathering**

1.  **Check Pod Status and Events:**
    *   **Command:** `kubectl get pods -n <namespace>`
        *   Identify the crashing pods. Look for `STATUS` as `CrashLoopBackOff` and increasing `RESTARTS` count.
    *   **Command:** `kubectl describe pod <pod-name> -n <namespace>`
        *   **Focus Areas:**
            *   `Events` section at the bottom: Look for `Warning` or `Error` events. Specifically, identify `OOMKilled` (Out Of Memory Killed), `FailedMount`, `Back-off restarting failed container`, or any image-related errors.
            *   `Containers` section: Verify the `Image` tag, `Command`, `Args`, `Environment` variables, `Mounts`, and `Liveness` / `Readiness` probes.
            *   `Resource` section: Check `Limits` and `Requests` for CPU and Memory.

2.  **Review Container Logs (Crucial Step):**
    *   **Command:** `kubectl logs <pod-name> -n <namespace>`
        *   This shows logs from the *current* crashing container instance. Look for application-specific error messages, stack traces, or initialization failures.
    *   **Command:** `kubectl logs <pod-name> -n <namespace> -p` (for previous container instance)
        *   This is essential, as the *reason for the crash* is often in the logs of the *previously terminated* container. Look for fatal errors, unhandled exceptions, or signs of resource exhaustion just before termination.
    *   **If multiple containers in a pod:** `kubectl logs <pod-name> -n <namespace> -c <container-name>`

**Phase 2: Deep Dive based on Initial Findings**

Based on the logs and events, narrow down the potential cause:

1.  **If `OOMKilled` is present in Events:**
    *   **Cause:** The container is running out of memory.
    *   **Troubleshooting:**
        *   Increase `resources.limits.memory` in the pod's container spec.
        *   Analyze application memory usage (if possible, using tools like `kubectl top pod <pod-name>` *if it starts briefly* or profiling in a staging environment).
        *   Ensure no memory leaks were introduced in the new code.
        *   **Best Practice:** Set `requests.memory` and `limits.memory` appropriately. `requests` should be what the application *needs* to start, `limits` should be its hard cap.

2.  **If Logs show "Configuration Error," "File Not Found," "Cannot Connect to DB," "Permission Denied," etc.:**
    *   **Cause:** Application configuration or environment issue.
    *   **Troubleshooting:**
        *   **Compare `Deployment` specs:** Get the current deployment YAML (`kubectl get deployment <deployment-name> -n <namespace> -o yaml`) and compare it with the previous working version (if available in version control). Look for differences in `env` variables, `volumeMounts`, `configMapRef`, `secretRef`, `command`, or `args`.
        *   **Check `ConfigMaps` and `Secrets`:**
            *   `kubectl get configmap <name> -n <namespace> -o yaml`
            *   `kubectl get secret <name> -n <namespace> -o yaml` (Be cautious with sensitive data in output).
            *   Ensure they are correctly mounted and contain the expected data.
        *   **Verify Image Content:** If the image changed, did it omit necessary files or environment variables that were implicitly available before?
        *   **Manual Check (if container can stay alive briefly):** `kubectl exec -it <crashing-pod-name> -n <namespace> -- <shell-command>` (e.g., `sh`, `bash`). Once inside, try to locate files, check environment variables (`env`), test database connectivity (`nc -vz <db-host> <db-port>` if netcat is present, or a simple `curl` if an HTTP service).

3.  **If Logs show "Application Crash," "Unhandled Exception," or No Output:**
    *   **Cause:** Likely a code bug or fundamental application startup failure.
    *   **Troubleshooting:**
        *   **Rollback:** Immediately roll back to the previous working deployment version to restore service.
        *   **Developer Review:** Engage the development team to review the code changes introduced in the new deployment. They can look for breaking changes, new dependencies, or logic errors.
        *   **Local Reproduction:** Attempt to reproduce the crash locally using the exact same image and configuration.
        *   **Debugging:** If the container can stay alive for a few seconds, attach a debugger (if supported by the application and image).

4.  **If `FailedMount` or `MountVolume.SetUp` errors in Events:**
    *   **Cause:** Issue with volume attachment or permissions.
    *   **Troubleshooting:**
        *   Verify `PersistentVolumeClaims` (PVCs), `PersistentVolumes` (PVs), `ConfigMaps`, and `Secrets` exist and are correctly configured.
        *   Check storage class, access modes, and capacity.
        *   Examine node logs for storage-related errors (`journalctl -u kubelet`).

5.  **If Liveness Probe Failures are Suspected:**
    *   **Cause:** The liveness probe is terminating the container before it's truly ready.
    *   **Troubleshooting:**
        *   Increase `initialDelaySeconds` for the liveness probe.
        *   Increase `timeoutSeconds` for the liveness probe.
        *   Consider temporarily disabling the liveness probe (in a controlled environment) to see if the application starts successfully without it.
        *   Ensure the liveness probe logic correctly reflects application health, not just readiness.

6.  **If External Dependency Issues are Suspected:**
    *   **Cause:** Application cannot reach critical external services.
    *   **Troubleshooting:**
        *   **Network Connectivity:**
            *   `kubectl exec -it <pod-name> -n <namespace> -- nc -vz <external-host> <port>` (if `netcat` is in image)
            *   `kubectl exec -it <pod-name> -n <namespace> -- curl <external-url>` (if `curl` is in image)
        *   **Check `NetworkPolicy`:** Review any recently applied or modified network policies in the namespace that might be blocking egress traffic.
        *   **Verify External Service Status:** Check the status of the database, message queue, or API Gateway.

**Phase 3: Monitoring and Best Practices**

1.  **Monitoring Strategies:**
    *   **Centralized Logging:** Implement a robust centralized logging solution (e.g., ELK Stack, Grafana Loki, Splunk, Datadog) to aggregate and analyze container logs from all pods. Set up alerts for specific error patterns or log levels (`ERROR`, `FATAL`).
    *   **Metrics & Dashboards:**
        *   Use Prometheus and Grafana (or similar solutions) to monitor Kubernetes and application metrics.
        *   **Key Metrics to Monitor:**
            *   `kube_pod_container_status_restarts_total`: Alert on a sudden increase in container restarts.
            *   `container_memory_usage_bytes`, `container_cpu_usage_seconds_total`: Monitor resource consumption and alert if nearing configured limits.
            *   Application-specific metrics: Track error rates, latency, and throughput during startup.
        *   **Kubernetes Events:** Monitor and alert on `Warning` and `Error` events within the cluster, especially those related to Pods, Deployments, and Nodes.
    *   **Health Checks:** Monitor the success/failure rates of liveness and readiness probes.

2.  **Best Practices for Preventing and Managing `CrashLoopBackOff`:**
    *   **Rollback Strategy:** Always have a well-defined and tested rollback strategy for deployments. `kubectl rollout undo deployment <deployment-name>` is your friend.
    *   **Progressive Rollouts:**
        *   **Canary Deployments / Blue-Green Deployments:** Gradually expose new versions to a small subset of users or infrastructure before a full rollout.
        *   **Kubernetes Rollout Strategy:** Use `maxUnavailable` and `maxSurge` in your `Deployment` spec to control the pace of rollout and availability during updates.
    *   **Automated Testing:** Implement comprehensive unit, integration, and end-to-end tests in your CI/CD pipeline to catch issues before deployment to production.
    *   **Staging Environment:** Deploy and test new versions thoroughly in a staging environment that closely mirrors production.
    *   **Version Control for Everything:** Keep all Kubernetes manifests, `ConfigMaps`, `Secrets` (templates), and application code in version control (GitOps). This makes comparing versions and identifying changes much easier.
    *   **Clear & Actionable Logging:** Ensure your application logs are verbose enough to provide clear indications of what went wrong during startup. Avoid generic error messages.
    *   **Define Resource Requests and Limits:** Provide realistic `resources.requests` and `resources.limits` for CPU and Memory to prevent OOMKills and ensure fair scheduling.
    *   **Robust Health Probes:** Configure `livenessProbe` and `readinessProbe` carefully. Liveness probes should check if the application is *alive*, readiness probes if it's *ready to serve traffic*. Use `initialDelaySeconds` and `timeoutSeconds` appropriately.
    *   **Post-Mortem & Root Cause Analysis (RCA):** After resolving the incident, conduct a post-mortem to understand the root cause, document lessons learned, and implement preventative measures.

---

By systematically working through these steps, you should be able to pinpoint the cause of the `CrashLoopBackOff` and restore service. Remember to document your findings and the resolution for future reference and continuous improvement.
--------------------------------------------------------------------------------
